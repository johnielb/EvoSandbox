---
title: AIML426 Project 2
author: Johniel Bocacao, 300490028
date: 24 September 2022
output: pdf_document
---

# 1. Evolutionary Programming

I chose to use fast EP after iteratively adding onto a basic EP implementation and finding it performed better at each iteration (admittedly without tuning parameters rigorously). This improvement may be due to the higher explorability associated with features of EP, such as the fat-tailed Cauchy distribution giving higher values in extreme values, and adaptive mutation enabling varying amounts of exploration and exploitation when needed.

I chose to use differential evolution due to its relative simplicity and elegance in its algorithm and its associated parameters and hyperparameters. Compared with evolutionary strategies, mutation levels doesn't need to be memorised and adjusted, and has the benefits of social learning (explore first, exploit when optimal) with its mutation operation.

The fitness function is simply the two given formulas as their optima are minima at 0. The solution encoding is a list of continuous floats, representing each dimension that contributes to the function. The stopping criterion is simply after 250 epochs, where all algorithms reach satisfactory fitness.

| Run                    | Mean    | Standard deviation |
|------------------------|---------|--------------------|
| **Rosenbrock, d=20**   |
| Fast EP                | 6779.77 | 16686.37           |
| Differential evolution | 4120.26 | 1400.46            |
| **Griewank, d=20**     |
| Fast EP                | 0.36103 | 0.22628            |
| Differential evolution | 0.45370 | 0.06709            |
| **Rosenbrock, d=50**   |
| Fast EP                | 5078777 | 2613442            |
| Differential evolution | 4307689 | 790528.8           |

At d=20, the Rosenbrock function had a 40% lower cost under differential evolution compared with fast EP. Differential evolution was also favourable due to its lower standard deviation (i.e. variability of results, thus reliably lower than fast EP), at 1400 - less than a tenth of that of fast EP, 16700.

The Griewank function showed different results, with a 20% lower cost under fast EP compared with differential evolution. What does remain the same across the two functions is differential evolution's low variability. This means some runs of fast EP have a greater cost (max 0.93657) than differential evolution (max 0.573862).

At d=50, the Rosenbrock function's cost has a much larger mean and standard deviation than at d=20, in the order of a thousand times more. This result demonstrates the difficulty of minimising the Rosenbrock function as dimensionality increases. This result may also reflect the need to tailor the hyperparameters, such as the population size, and stopping criterion further for a problem of this complexity.

Like at d=20, the Rosenbrock function at d=50 also performed better with differential evolution, with a 15% lower cost. This result reinforces the conclusion that this technique is better than fast EP for this problem in particular.

# 2. Estimation of Distribution Algorithm
* Determine the proper individual representation and explain the reasons.
* Design the proper fitness function and justify its use.
* Design and implement your EDA algorithm.
* Set proper algorithm parameters, such as population size, individual selection criteria, and learning rate (subject to your choice of EDA variations).
* For each knapsack problem instance, run your EDA implementation for 5 times with different random seeds. Present the mean and standard deviation of your EDA in the 5 runs.
* Draw the convergence curve of your EDA implementation for each knapsack problem instance. The x-axis of your convergence curve represents the number of generations. The y-axis stands for the average fitness of the best solutions in the population of the x-th generation from the 5 runs. Discuss your convergence curve and draw your conclusions.

![Convergence curves oer dataset by seed](out/fig1.png)

# 3. Cooperative Co-evolution Genetic Programming

