---
title: AIML426 Project 2
author: Johniel Bocacao, 300490028
date: 24 September 2022
output: pdf_document
---

# 1. Evolutionary Programming

I chose to use fast EP after iteratively adding onto a basic EP implementation and finding it performed better at each iteration (admittedly without tuning parameters rigorously). This improvement may be due to the higher explorability associated with features of EP, such as the fat-tailed Cauchy distribution giving higher values in extreme values, and adaptive mutation enabling varying amounts of exploration and exploitation when needed.

I chose to use differential evolution due to its relative simplicity and elegance in its algorithm and its associated parameters and hyperparameters. Compared with evolutionary strategies, mutation levels doesn't need to be memorised and adjusted, and has the benefits of social learning (explore first, exploit when optimal) with its mutation operation.

The fitness function is simply the two given formulas as their optima are minima at 0. The solution encoding is a list of continuous floats, representing each dimension that contributes to the function. The stopping criterion is simply after 250 epochs, where all algorithms reach satisfactory fitness.

| Run                    | Mean    | Standard deviation |
|------------------------|---------|--------------------|
| **Rosenbrock, d=20**   |
| Fast EP                | 6779.77 | 16686.37           |
| Differential evolution | 4120.26 | 1400.46            |
| **Griewank, d=20**     |
| Fast EP                | 0.36103 | 0.22628            |
| Differential evolution | 0.45370 | 0.06709            |
| **Rosenbrock, d=50**   |
| Fast EP                | 5078777 | 2613442            |
| Differential evolution | 4307689 | 790528.8           |

At d=20, the Rosenbrock function had a 40% lower cost under differential evolution compared with fast EP. Differential evolution was also favourable due to its lower standard deviation (i.e. variability of results, thus reliably lower than fast EP), at 1400 - less than a tenth of that of fast EP, 16700.

The Griewank function showed different results, with a 20% lower cost under fast EP compared with differential evolution. What does remain the same across the two functions is differential evolution's low variability. This means some runs of fast EP have a greater cost (max 0.93657) than differential evolution (max 0.573862).

At d=50, the Rosenbrock function's cost has a much larger mean and standard deviation than at d=20, in the order of a thousand times more. This result demonstrates the difficulty of minimising the Rosenbrock function as dimensionality increases. This result may also reflect the need to tailor the hyperparameters, such as the population size, and stopping criterion further for a problem of this complexity.

Like at d=20, the Rosenbrock function at d=50 also performed better with differential evolution, with a 15% lower cost. This result reinforces the conclusion that this technique is better than fast EP for this problem in particular.

The Griewank function performed significantly better in both algorithms than either of the Rosenbrock function runs. Both functions have their global minimum fitness at 0: the Rosenbrock function only converged at the 10^3-10^6 magnitude with the same amount of generations as the Griewank function did, which reached the 10^-1 magnitude. This result may be due to the greater amount of local minima close to 0 in the Griewank function, while the Rosenbrock function is shaped as a slow and narrow valley to 0. Neither necessarily are unsuccessful, as we may find with more generations (which would require more computational power than this assignment necessitates) both take similar times to get to the global minimum 0, with different obstacles to get there.

# 2. Estimation of Distribution Algorithm
We start with a randomly initialised population, with **500** individuals represented by a list of Booleans. This population size was chosen empirically after determining smaller sizes (that were successful in other evolutionary algorithms) did not enable a convergence to the stated optimal value. This representation encodes what an individual is: a series of Boolean choices whether to include an item in the knapsack, rather than a string of ASCII integers, for example. The individual selection criteria was a simple best-M selection, with elitism pre-populating the next generation with the top 5% of the parent generation to ensure that each generation's best individuals are not worse than the last, continuously improving each generation.

Fitness is determined by summing the candidateâ€™s value and penalising by how much the sum of its weight overshoots capacity (**alpha = 50**, constant). Instead of rejecting such an individual, this penalty promotes diversity in the generation, giving the algorithm more material.

| Seed                  | 10_269 | 23_10000 | 100_995 |
|-----------------------|--------|----------|---------|
| 0                     | 295    | 9767     | 1514    |
| 1                     | 295    | 9767     | 1513    |
| 2                     | 295    | 9767     | 1514    |
| 3                     | 295    | 9767     | 1514    |
| 4                     | 295    | 9767     | 1514    |
| Mean                  | 295    | 9767     | 1513.8  |
| Optima                | 295    | 9767     | 1514    |
| Deviation from optima | 0      | 0        | 0.2     |
| Standard deviation    | 0      | 0        | 0.4     |

![Convergence curves per dataset by seed](out/fig1.jpg)

All files and seeds converged to the optimal value (with the slight exception of 100_995 seed 1 missing by 1). The
simple 10_269 dataset had all seeds converging to the optimum 295 by the third epoch.

The more complex 23_10000 dataset took around 15 epochs to converge to the optimum 9767. Unlike the other two, each
seed's run had noticeable variability. This result reflects the similar (in weight and value) candidates in the set
providing fewer options in smoothly converging to the optimum.

This shortcoming is not evident in the 100_995 dataset with a greater variety (in weight and value) of candidates
providing an obvious path to the optimum for all seeds (although seed 1's fitness was off by one, it still followed the
same trajectory).

# 3. Cooperative Co-evolution Genetic Programming
For both subpopulations, the terminal set used in this problem consisted of a random float or a random bool (True or
False), and the input of the function x. This set captures all the possible variable/terminal types in the regression problem.

For both subpopulations, the function set in this problem added all the functions sufficient to capture the regression
problem. The only assumption we can use is that it is a piecewise function, theoretically we don't know which functions
are used in each piece of the function:
- Add, subtract, and multiply, divide with protection if a zero is used as the denominator.
- Sine only. Cosine is just a phase-shifted sine, so a problem with cosine (which this problem doesn't use) can be conveyed with sine.
- Square. I initially tried a power function, but became too _complex_ to handle with negative bases.


Design the fitness function and the fitness evaluation method for each sub-population.
Set the necessary parameters, such as sub-population size, maximum tree depth, termination criteria, crossover and mutation rates.
Run the implemented CCGP for 5 times with different random seeds. Report the best genetic programs (their structure and performance) of each of the 5 runs. Present your observations and discussions and draw your conclusions.

| Seed | Best fitness (MSE, minimised) | Corresponding node length |
|------|-------------------------------|---------------------------|
| 0    |                               |                           |
| 1    |                               |                           |
| 2    |                               |                           |
| 3    |                               |                           |
| 4    |                               |                           |
| Mean |                               |                           |
| SD   |                               |                           |
